##### Basic Points
Brains can:
- recognise images/objects/abstract symbolic information
- retrieve information based on partial descriptions
- organise information

Computers can:
- do arithmetic and arithmetic logic
- deductive logic
- retrieve information based on arbitrary features

![[Pasted image 20231120123721.png]]

##### Perceptron
A perceptron is an algorithm for supervised learning of binary classifiers. This algorithm enables neurones to learn and process elements in the training set one at a time.

![[Pasted image 20231120123738.png]]

A perceptron is a neural network composed of a single layer feed-forward network using threshold activation functions. Feed-forward means that all the interconnections between the layers propagate forward to the next layer.

The simple perceptron uses the threshold activation function with a bias and thus has a binary output. The binary output perceptron has two possible outputs: 0 and 1. It is trained by supervised learning and can only classify input patterns that are linearly separable.

Training is accomplished by initialising the weights and bias to small random values and then presenting input data to the network. The output ($y$) is compared to the target output ($t = 0$ or $t = 1$), and the weights are adapted according to Hebb's training rule.

The simple single layer perceptron can separate linearly separable inputs but will fail if the inputs are otherwise. One such example of linearly non-separable inputs is the exclusive-or (XOR) problem. Linearly non-separable patterns, such as those of the XOR problem, can be separated with multilayer networks.

> [!Definition] Limitations Summary
> Perceptrons are limited in that they can only separate linearly separable patterns and that they only have a binary output.

Many of the limitations of the simple perceptron can be solved with multi-layer architectures, non-binary activation functions, and more complex training algorithms.

##### Multiple Perceptron
Multilayer perceptrons with threshold activation functions are not that useful because they can't be trained with the perceptron learning rule and since the functions are not differentiable, they can't be trained with gradient descent algorithms.

###### Activation Functions
The activate function is generally non-linear. Linear functions are limited because the output is simply proportional to the input.
![[Pasted image 20231030172857.png]]

![[Pasted image 20231030173051.png]]
*Multilayer architecture*

##### Feedforward Neural Networks
The basic structure of a feedforward neural network is as follows:
![[Pasted image 20231030173803.png|test]]
The learning rule modifies the weights according to the input the patterns are presented with. In a sense, ANNs learn by example as do their biological counterparts.

When the desired outputs are known we have supervised learning or learning with a teacher.
![[Pasted image 20231030174005.png]]
*Multiple layers in the feedforward network*

##### Overview of the back-propagation
1. a set of examples for training the network is assembled. Each case consists of a problem statement (which represents the input into the network) and the corresponding solution (which represents the desired output from the network).
2. the input data is entered into the network via the input layer.
3. each neurone in the network processes the input data with the resultant values steadily "percolating" through the network, layer by layer, until a result is generated by the output layer
4. the actual output of the network is compared to expected output for that particular input. This results in an *error value*. The connection weights in the network are gradually adjusted, working backwards from the output layer, through the hidden layer, and to the input layer, until the correct output is produced. Fine tuning the weights in this way has the effect of teaching the network how to produce the correct output for a particular input $\implies$ the network *learns*

![[Pasted image 20231120123913.png]]

##### The Learning Rule
The delta rule is often utilised by the most common class of ANNs called 'backpropagational neural networks'.
![[Pasted image 20231120124343.png]]
When a neural network is initially presented with a pattern it makes a random guess as to what it might be. It then sees how far its answer was from the actual one and makes an appropriate adjustment to its connection weights.

###### Inside the Delta Rule
Backpropagation performs a gradient descent within the solution's vector space towards a global minimum. The error surface itself is a hyperparaboloid but is seldom smooth as is depicted in the graphic below.
![[Pasted image 20231120124536.png]]
Indeed, in most problems, the solution space is quite irregular with numerous pits and hills which may cause the network to settle down in a *local minimum* which is not the best overall solution.


### Multilayer Neural Network
Neural networks with one or more hidden layers are called *multilayer neural networks* or multilayer perceptrons (MLP). Normally, each hidden layer of a network uses the same type of activation function.

The output activation function is either sigmoidal or linear. The output of a sigmoidal neurone is constrained $[-1, 1]$ for a hyperbolic tangent and $[0, 1]$ for a logarithmic sigmoidal neurone.

A linear output neurone is not constrained and can output a value of any magnitude.

![[Pasted image 20231120124855.png]]
*Sigmoid function vs hyperbolic tangent function*

It has been proved that the standard feedfoward multilayer perceptron (MLP) with a single non-linear hidden layer (sigmoidal neurones) can approximate any continuous function to any desired degree of accuracy over a compact set.

In order to be a universal approximation, the hidden layer of a multilayer perceptron is usually a sigmoidal neurone. A linear hidden layer is rarely used because any two linear transformations can be represented as one linear transformation (matrix algebra).