## NFAs $\implies$ Regular Grammars
Every NFA can be converted into a corresponding regular grammar:
- each  state (node) of the NFA is associated with a non-terminal symbol of the grammar, the initial state is associated with the start symbol.
- every transition is associated with a grammar production e.g if the state 1 corresponds to the non-terminals $A$, then
	  $T(S, a) = 1 \Leftrightarrow S \rightarrow aA$.
- every final state has an additional production, $A \rightarrow \lambda$.

Example: find a grammar corresponding to the following NFA:
![[Pasted image 20231010100413.png]]

Applying the previous rules we get:
	$S \rightarrow aA \:|\: aC$
	$A \rightarrow bB$
	$B \rightarrow \lambda$
	$C \rightarrow aC \:|\: \lambda$
This may not be the simplest grammar, but it does work!

## Testing for regular languages
Suppose we want to find a DFA or NFA to recognise the following language
	$\{a^nb^n \:|\: n \gt 0\}$ 
The automaton would have to remember the number of $a$'s it has seen, which might be arbitrarily large - this is impossible for a machine with a finite number of possible states. Any algorithm will break down when $n$ exceeds the number of states of the machine.

So the language is not regular, but can we prove it? Equivalently, can we prove a given language cannot be recognised by any finite automaton?

### Pumping lemma
> [!info] Theorem (The pigeonhole principle)
> If we put $n$ pigeons into $m$ pigeonholes ($n \gt m$), then at least one pigeonhole must have more than one pigeon.

If the input string is long enough (e.g greater than the number of states of the minimum state DFA), then there must be at least one state $Q$ which is visited more than one. Thus, there must be at least one closed loop, which begins and ends at state $Q$, and a particular string, $y$, which corresponds to this loop.

We can represent the situation as such:
![[Pasted image 20231010101130.png]]

Each dotted arrow represents a path that may contain other states of the DFA:
- $x$ is a string of letters which the automaton reads from the start to state $Q$
- $y$ is the string of letters around the closed loop
- $z$ is a string of letters from $Q$ to a final state
We know that the string $xyz$ is accepted. But this means the DFA must also accept $xz, xyyz, xyyyz, ..., x\underbrace{y...y}_kz, ...$ where the amount of middle $y$'s is $k$ (the middle string is "pumped").

More formally,
> [!info] Theorem (Pumping Lemma)
> Let $L$ be an infinite regular language accepted by a DFA with $m$ states. Then any string $w$ in $L$ with at least $m$ symbols can be decomposed as $w = xyz$ with $|xy| \leq m$ and $|y| \geq 1$ such that
> 	<p  class="tab"/>$w_i = x\underbrace{y...y}_iz$
> 
> is also in $L$ for all $i = 0, 1, 2, ...$

If a language fails to satisfy the pumping lemma, then it cannot be regular. But simply satisfying the pumping lemma does not prove the language is regular. You can use it to prove a language is NOT regular, but not necessarily that a language IS regular.

- If $L$ is an infinite regular language ($A$) $\implies$ all strings of $L$ must satisfy the pumping lemma (pre-described structure) ($B$)
- $A \implies B \equiv \neg{B} \implies \neg{A}$
  if an infinite language fails to satisfy the pumping lemma (there exists a string without pre-described structure), then it cannot be regular
- $A \implies B \not\equiv B \implies A$ 
  but even if all strings satisfy the pumping lemma (have pre-described structure), it does not prove the language is regular

### Proof by contradiction
We can prove it by contradiction using the pumping lemma.

Assume that $L$ is regular $\implies L$ is accepted by a unique DNF with $m$ states $\implies$ any string from $L$ of length at least $m$ can be decomposed as
	$xyz$ with $|xy| \leq m$ and $|y| \geq 1$ 
such that $x\underbrace{y...y}_iz$ is also in $L$ for all $i = 0, 1, 2, ...$
Take $n \gt m$ and the string $a^nb^n$ from $L \implies$ the substring $y$ (of the length $k$) must consist entirely of $a$'s.

Due to the pumping lemma, the string $a^{n-k}b^n$ must also be from $L$, which is not true. Therefore, the assumption that $L = \{a^nb^n, n \geq 0\}$ is regular must be false.

### Context-Free Languages
The grammar of the regular language is too strict and doesn't allow the description of many simple languages e.g $L = \{a^nb^n, n\geq 0\}$.

We can add, step by step, more freedom to the grammar production to define other families of languages.

> [!info] Definition
> A grammar $G$ is said to be *context-free* if all of its productions have the form
> 	<p  class="tab"/>$N \rightarrow \alpha$ 
> 
> where $N$ is a non-terminal and $\alpha$ is any string over the alphabet of terminals and non-terminals.

All regular languages are context-free, but not all context-free languages are regular.

Example: the grammar over the alphabet $\{a, b\}$ with productions
	$S \rightarrow aSb \:|\: \lambda$ 
is context-free. The language generated by this grammar:
	$L = \{a^nb^n, n \geq 0\}$

Example: the grammar over the alphabet $\{a, b\}$ with productions
	$S \rightarrow aSa \:|\: bSb \:|\: \lambda$
is context-free. The language generated by this grammar:
	$L = \{ww^R : w \in \{a, b\}^\ast\}$

Context-free comes from the requirement that all productions contain a single non-terminal on the left. When this is the case, any production $N \rightarrow \alpha$ can be used in derivation without regard to the "context" in which the grammatical symbol $N$ appears.

Therefore, we can use this rule to make the following derivation step:
	$aNb \rightarrow a{\alpha}b$
whatever surrounds the $N$.

### Non-context-free grammars
A grammar that is not context-free must contain a production whose left hand side is a string of two or more symbols.

Example: the production $Nc \rightarrow \alpha$ is not part of any context-free grammar.

A derivation that uses this production can replace the non-terminal $N$ only in a "context" that has $c$ on the right, e.g
	$aNc \rightarrow a\alpha$ 

### Context-free and programming languages
The process of taking text and translating it for the computer is called "parsing", and consists of two parts:
1. The tokenizer - also called a "lexer" or a "scanner". The tokenizer takes the source text and breaks it into the reserved words, constants, identifiers, and symbols that are defined in the language (DFA).
2. These tokens are subsequently passed to the actual parser which analyzes the series of tokens and then determines when one of the language's syntax rules is complete.

### Context-sensitive languages
Context-sensitive grammars allow even more complex transitions.

> [!info] Definition
> A context-sensitive grammar is a grammar whose productions are of the form
> 	<p  class="tab"/>${\alpha}A\beta \rightarrow {\alpha}{\gamma}\beta$
> 
> where $\alpha, \beta \in (N \cup T)^\ast, A \in N; \gamma \in (N \cup T)^+$ and a rule of the form $S \rightarrow \lambda$ is allowed if the start symbol $S$ does not appear on the right-hand side of any rule.

Such a language generated by a grammar as above is called a context-sensitive language.

Every context-free grammar is also context-sensitive, but not every context-sensitive language is context-free.

### Phrase structure grammars
The most general grammars which we can define are Phrase Structure Grammars, or Unrestricted Grammars.

> [!info] Definition
> A phase structure grammar is a grammar whose productions are of the form
> 	<p  class="tab"/>$\alpha \rightarrow \beta$
> 
> where $\alpha \in (N \cup T)^+$ and $\beta \in (N \cup T)^\ast$

That is, $\alpha$ and $\beta$ can be any sequence of non-terminals and terminals, but $\beta$ could also be $\lambda$.

The phrase structure grammars generate the most general class of language, called recursively enumerable.

### Chomsky hierarchy
We can form a hierarchy of languages (called the Chomsky hierarchy), where one language includes the ones below it:
0. Phrase Structure
1. Context-Sensitive
2. Context-Free
3. Regular
There are also infinite languages that cannot be generated by a finite set of recursive productions. These are known as non-grammatical languages.

![[Pasted image 20231010104711.png]]
As the grammar rules become less restrictive, the language classes grow, but they include the simpler languages as subsets.

